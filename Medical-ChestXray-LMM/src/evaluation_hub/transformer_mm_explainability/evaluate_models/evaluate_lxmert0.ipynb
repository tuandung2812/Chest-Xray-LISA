{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/12T/02_duong/miniconda3/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-27 03:51:01.599343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 03:51:02.175920: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-27 03:51:02.175981: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-27 03:51:02.175986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/duong/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/12T/02_duong/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
      "  0%|          | 0/57132 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/12T/02_duong/Medical-ChestXray-Dataset-for-LMM/src\")\n",
    "\n",
    "import os\n",
    "from dotmap import DotMap\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation_hub.load_data.load_probmed_data import load_probmed_data\n",
    "from model_hub.lxmert.lxmert_0_explained.predict_lxmert_0_explained import LXMERTExplainedHandler\n",
    "\n",
    "\n",
    "def evaluate_lxmert0(\n",
    "    filepath_metadata,\n",
    "    dirpath_images,\n",
    "    dirpath_output,\n",
    "):\n",
    "    os.makedirs(dirpath_output, exist_ok=True)\n",
    "    filepath_output = os.path.join(dirpath_output, f\"{os.path.basename(filepath_metadata).split('.')[0]}_lxmert0.json\")\n",
    "    context0 = {\n",
    "        \"model_id\": \"lxmert_0\",\n",
    "        \"use_lrp\": True,\n",
    "    }\n",
    "    context = DotMap(context0)\n",
    "    handler = LXMERTExplainedHandler(context)\n",
    "    metadata = load_probmed_data(\n",
    "        filepath_metadata=filepath_metadata\n",
    "    )\n",
    "\n",
    "    for sample0 in tqdm(metadata):\n",
    "        sample = sample0\n",
    "        if sample[\"image_type\"] != \"x-ray_chest\":\n",
    "            continue\n",
    "        filepath_image = os.path.join(dirpath_images, sample[\"image\"])\n",
    "        question = sample[\"question\"].replace('<image>', '').strip()\n",
    "        handler_input = {\n",
    "            \"filepath_image\": filepath_image,\n",
    "            \"user_prompt\": question\n",
    "        }\n",
    "        R_t_t, R_t_i = handler.handle(handler_input)\n",
    "        visualize(R_t_t, R_t_i, handler.model, dirpath_output, filepath_image, question)\n",
    "        \n",
    "\n",
    "def visualize(\n",
    "    R_t_t, \n",
    "    R_t_i,\n",
    "    model_lrp,\n",
    "    dirpath_output,\n",
    "    filepath_image,\n",
    "    question\n",
    "):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import torch\n",
    "    from captum.attr import visualization\n",
    "    from model_hub.lxmert.lxmert_0_explained.modeling import vqa_utils as utils\n",
    "\n",
    "    image_scores = R_t_i[0]\n",
    "    text_scores = R_t_t[0]\n",
    "    VQA_URL = \"https://raw.githubusercontent.com/airsplay/lxmert/master/data/vqa/trainval_label2ans.json\"\n",
    "    vqa_answers = utils.get_data(VQA_URL)\n",
    "\n",
    "    def save_image_vis(image_file_path, bbox_scores):\n",
    "        bbox_scores = image_scores\n",
    "        _, top_bboxes_indices = bbox_scores.topk(k=1, dim=-1)\n",
    "        img = cv2.imread(image_file_path)\n",
    "        img0 = img.copy()\n",
    "        mask = torch.zeros(img.shape[0], img.shape[1])\n",
    "        for index in range(len(bbox_scores)):\n",
    "            [x, y, w, h] = model_lrp.bboxes[0][index]\n",
    "            curr_score_tensor = mask[int(y):int(h), int(x):int(w)]\n",
    "            new_score_tensor = torch.ones_like(curr_score_tensor)*bbox_scores[index].item()\n",
    "            mask[int(y):int(h), int(x):int(w)] = torch.max(new_score_tensor,mask[int(y):int(h), int(x):int(w)])\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "        mask = mask.unsqueeze_(-1)\n",
    "        mask = mask.expand(img.shape)\n",
    "        img = img * mask.cpu().data.numpy()\n",
    "        concat = np.concatenate((img0, img), axis=1)\n",
    "        cv2.imwrite(os.path.join(dirpath_output, 'new.jpg'), concat)\n",
    "    \n",
    "    save_image_vis(filepath_image, image_scores)\n",
    "    orig_image = Image.open(model_lrp.image_file_path)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "    axs[0].imshow(orig_image)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('original')\n",
    "    [[']]]]]']]\n",
    "    masked_image = Image.open(os.path.join(dirpath_output, 'new.jpg'))\n",
    "    axs[1].imshow(masked_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('masked')\n",
    "\n",
    "    text_scores = (text_scores - text_scores.min()) / (text_scores.max() - text_scores.min())\n",
    "    vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,model_lrp.question_tokens,1)]\n",
    "    data = visualization.visualize_text(vis_data_records)\n",
    "    print(question)\n",
    "    print(\"ANSWER:\", vqa_answers[model_lrp.output.question_answering_score.argmax()])\n",
    "    plt.show(block = False)\n",
    "\n",
    "\n",
    "evaluate_lxmert0(\n",
    "    filepath_metadata=\"/mnt/12T/02_duong/data-center/ProbMed/test/test.json\",\n",
    "    dirpath_images=\"/mnt/12T/02_duong/data-center/ProbMed/test\",\n",
    "    dirpath_output=\"/mnt/12T/02_duong/data-center/Medical-ChestXray-Dataset-for-LMM-Data/evaluation_hub/transformer_mm_explainability\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
