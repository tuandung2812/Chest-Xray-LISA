
CondaError: Run 'conda init' before 'conda activate'

/home/user01/miniconda3/envs/lisa_1/compiler_compat/ld: cannot find -lcufile: No such file or directory
collect2: error: ld returned 1 exit status
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/user01/.netrc
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/wandb/run-20241021_014217-czibll4f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run full_medsam_vindr_llavamed_bs_16_lr_5e-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tuandung2812alt1-hanoi-university-of-science-and-technology/VinDr%20VQA%20-%20Debug
wandb: üöÄ View run at https://wandb.ai/tuandung2812alt1-hanoi-university-of-science-and-technology/VinDr%20VQA%20-%20Debug/runs/czibll4f
Loading checkpoint shards:   0%|                                                                    | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                             | 1/4 [00:00<00:00,  7.82it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 2/4 [00:00<00:00,  8.36it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 3/4 [00:00<00:00,  8.19it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  8.51it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  8.37it/s]
Traceback (most recent call last):
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/train_text.py", line 728, in <module>
    main(sys.argv[1:])
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/train_text.py", line 427, in main
    train_iter = train(
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/train_text.py", line 515, in train
    output_dict = model(**input_dict)
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/peft/peft_model.py", line 922, in forward
    return self.base_model(
  File "/home/user01/miniconda3/envs/lisa_1/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/model/LISA.py", line 173, in forward
    return self.model_forward(**kwargs)
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/model/LISA.py", line 343, in model_forward
    gt_mask.shape[0] == pred_mask.shape[0]
AssertionError: gt_mask.shape: torch.Size([1, 3075, 2718]), pred_mask.shape: torch.Size([5, 3075, 2718])
wandb: - 0.028 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.077 MB uploadedwandb: | 0.077 MB of 0.077 MB uploadedwandb: üöÄ View run full_medsam_vindr_llavamed_bs_16_lr_5e-5 at: https://wandb.ai/tuandung2812alt1-hanoi-university-of-science-and-technology/VinDr%20VQA%20-%20Debug/runs/czibll4f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/tuandung2812alt1-hanoi-university-of-science-and-technology/VinDr%20VQA%20-%20Debug
wandb: Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241021_014217-czibll4f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
