
CondaError: Run 'conda init' before 'conda activate'

Traceback (most recent call last):
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1764, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py", line 44, in <module>
    from ...modeling_flash_attention_utils import _flash_attention_forward
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 27, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 10, in <module>
    import flash_attn_2_cuda as flash_attn_cuda
ModuleNotFoundError: No module named 'flash_attn_2_cuda'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/train_text.py", line 9, in <module>
    from model.LISA import LISAForCausalLM
  File "/home/user01/aiotlab/dung_paper/groundingLMM/LISAMed/model/LISA.py", line 6, in <module>
    from transformers import BitsAndBytesConfig, CLIPVisionModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1755, in __getattr__
    value = getattr(module, name)
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1754, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/user01/miniconda3/envs/glamm_2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1766, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):
No module named 'flash_attn_2_cuda'
